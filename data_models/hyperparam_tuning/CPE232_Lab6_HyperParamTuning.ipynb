{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 6: Introducing Hyperparameter Tuning\n",
    "\n",
    "Objectives:\n",
    "- To gain hands-on experience tuning parameters\n",
    "- To implement concepts related to Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and Prepare Data\n",
    "\n",
    "We're going to use iris dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Split into training and testing sets (80-20 split)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use decision tree again! (Don't forget it yet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a Decision Tree model\n",
    "dt = DecisionTreeClassifier(random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameter Tuning Using GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters from GridSearchCV: {'criterion': 'entropy', 'max_depth': 5, 'min_samples_leaf': 4, 'min_samples_split': 2}\n"
     ]
    }
   ],
   "source": [
    "# Define parameter grid for tuning\n",
    "param_grid = {\n",
    "    'criterion': ['gini', 'entropy'],\n",
    "    'max_depth': [3, 5, 10, None],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "# Apply GridSearchCV\n",
    "grid_search = GridSearchCV(dt, param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get best parameters\n",
    "print(\"Best Parameters from GridSearchCV:\", grid_search.best_params_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameter Tuning Using RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters from RandomizedSearchCV: {'criterion': 'entropy', 'max_depth': None, 'min_samples_leaf': 3, 'min_samples_split': 3}\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import randint\n",
    "\n",
    "# Define parameter distributions for tuning\n",
    "param_dist = {\n",
    "    'criterion': ['gini', 'entropy'],\n",
    "    'max_depth': [3, 5, 10, None],\n",
    "    'min_samples_split': randint(2, 20),\n",
    "    'min_samples_leaf': randint(1, 10)\n",
    "}\n",
    "\n",
    "# Apply RandomizedSearchCV\n",
    "random_search = RandomizedSearchCV(dt, param_dist, n_iter=10, cv=5, scoring='accuracy', n_jobs=-1, random_state=42)\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "# Get best parameters\n",
    "print(\"Best Parameters from RandomizedSearchCV:\", random_search.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy with Best GridSearchCV Model: 1.0000\n"
     ]
    }
   ],
   "source": [
    "# Train a Decision Tree using the best parameters from GridSearchCV\n",
    "best_dt = grid_search.best_estimator_\n",
    "y_pred = best_dt.predict(X_test)\n",
    "\n",
    "# Compute accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Test Accuracy with Best GridSearchCV Model: {accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tasks:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use wine dataset :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_wine\n",
    "\n",
    "# Load dataset\n",
    "wine = load_wine()\n",
    "X = wine.data\n",
    "y = wine.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(178, 13)\n",
      "(178,)\n"
     ]
    }
   ],
   "source": [
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 1: Train-Test Split**\n",
    "\n",
    "- Split the Wine dataset into training and testing sets using an 80-20 split.\n",
    "- What is the role of the train-test split in evaluating a machine learning model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.25,random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we use `X_train`, `y_train`, to evaluate the learnable params and `X_test`, `y_test` to test the model for evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 2: Cross-Validation**\n",
    "\n",
    "- Implement cross-validation using GridSearchCV and RandomizedSearchCV.\n",
    "- How does cross-validation help in providing a more reliable estimate of the model's performance?\n",
    "- Discuss how cross-validation improves the results and prevents overfitting compared to a simple train-test split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'criterion': 'gini', 'max_depth': 3, 'min_samples_leaf': 1, 'min_samples_split': 2}\n",
      "{'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': None, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'random_state': 42, 'splitter': 'best'}\n",
      "acc of best_grid : 0.9555555555555556\n",
      "acc of best_random : 0.9333333333333333\n",
      "acc of og : 0.9555555555555556\n"
     ]
    }
   ],
   "source": [
    "# Write your code here\n",
    "dt = DecisionTreeClassifier(random_state=42)\n",
    "param_grid = {\n",
    "    'criterion': ['gini', 'entropy'],\n",
    "    'max_depth': [3, 5, 10, None],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "# Apply GridSearchCV\n",
    "grid_search = GridSearchCV(dt, param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "grid_search.fit(X_train,y_train)\n",
    "print(grid_search.best_params_)\n",
    "param_dist = {\n",
    "    'criterion': ['gini', 'entropy'],\n",
    "    'max_depth': [3, 5, 10, None],\n",
    "    'min_samples_split': randint(2, 20),\n",
    "    'min_samples_leaf': randint(1, 10)\n",
    "}\n",
    "\n",
    "# Apply RandomizedSearchCV\n",
    "random_search = RandomizedSearchCV(dt, param_dist, n_iter=10, cv=5, scoring='accuracy', n_jobs=-1, random_state=42)\n",
    "random_search.fit(X_train,y_train)\n",
    "dt.fit(X_train,y_train)\n",
    "print(dt.get_params())\n",
    "\n",
    "best_grid = grid_search.best_estimator_\n",
    "grid_pred = best_grid.predict(X_test)\n",
    "best_random = random_search.best_estimator_\n",
    "random_pred = best_random.predict(X_test)\n",
    "\n",
    "pred = dt.predict(X_test)\n",
    "print(f\"acc of best_grid : {accuracy_score(y_test,grid_pred)}\")\n",
    "print(f\"acc of best_random : {accuracy_score(y_test,random_pred)}\")\n",
    "print(f\"acc of og : {accuracy_score(y_test,pred)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Cross validation continuously training model over multiple test set, so we are sure that the result that we get from the CV is like average result from multiple training set.\n",
    "- When doing traditional `train_test_split`, model could prone to overfit because the training set could have some certain pattern that just occur in that dataset. But for CV, it would train over multiple training set. So model would get generalize over multiple dataset.\n",
    "\n",
    "> the answers above are just theoritical answers, but don't seem to show here. Anyway, it could be caused by other factor such as size of dataset and variance of data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 3: Hyperparameter Tuning**\n",
    "\n",
    "- Use GridSearchCV to find the best hyperparameters by exhaustively searching through the parameter grid.\n",
    "- Use RandomizedSearchCV to sample a fixed number of combinations of hyperparameters.\n",
    "- Compare the results from both methods in terms of accuracy and computation time.\n",
    "- Discuss the trade-offs between GridSearchCV and RandomizedSearchCV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "208 ms ± 15.1 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
      "39 ms ± 1.31 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "# Write your code here\n",
    "import time\n",
    "\n",
    "dt = DecisionTreeClassifier(random_state=42)\n",
    "param_grid = {\n",
    "    'criterion': ['gini', 'entropy'],\n",
    "    'max_depth': [3, 5, 10, None],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "# Apply GridSearchCV\n",
    "grid_search = GridSearchCV(dt, param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "%timeit grid_search.fit(X_train,y_train)\n",
    "\n",
    "\n",
    "param_dist = {\n",
    "    'criterion': ['gini', 'entropy'],\n",
    "    'max_depth': [3, 5, 10, None],\n",
    "    'min_samples_split': randint(2, 20),\n",
    "    'min_samples_leaf': randint(1, 10)\n",
    "}\n",
    "random_search = RandomizedSearchCV(dt, param_dist, n_iter=10, cv=5, scoring='accuracy', n_jobs=-1, random_state=42)\n",
    "%timeit random_search.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`GridSearchCV` would take more time, since the search space it large and it would go through every point in the search space\n",
    "and as `%timeit` function show, `RandomSearchCV` take much less time, because it only just search only certain point specified by `n_iters`\n",
    "\n",
    "the trade of would be that `RandomizedSearchCV` would use less time, but would find only minimum from those sampled point, could be global maximum, but not certain. Where `GridGridSearchCV` will surely find the global maximum from all combination of hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 4: Model Evaluation**\n",
    "\n",
    "- Using the best model from GridSearchCV or RandomizedSearchCV, make predictions on the test set and calculate the accuracy.\n",
    "- Compare the performance of the tuned model with a baseline Decision Tree model (without hyperparameter tuning).\n",
    "- How does hyperparameter tuning affect the accuracy of the Decision Tree model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc of best_grid : 0.9555555555555556\n",
      "acc of best_random : 0.9333333333333333\n",
      "acc of og : 0.9555555555555556\n",
      "{'criterion': 'gini', 'max_depth': 3, 'min_samples_leaf': 1, 'min_samples_split': 2}\n",
      "{'criterion': 'gini', 'max_depth': None, 'min_samples_leaf': 5, 'min_samples_split': 5}\n",
      "{'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': None, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'random_state': 42, 'splitter': 'best'}\n",
      "randomly change hyperparam\n",
      "randomly change param : 0.9777777777777777\n"
     ]
    }
   ],
   "source": [
    "dt.fit(X_train,y_train)\n",
    "best_grid = grid_search.best_estimator_\n",
    "grid_pred = best_grid.predict(X_test)\n",
    "best_random = random_search.best_estimator_\n",
    "random_pred = best_random.predict(X_test)\n",
    "pred = dt.predict(X_test)\n",
    "print(f\"acc of best_grid : {accuracy_score(y_test,grid_pred)}\")\n",
    "print(f\"acc of best_random : {accuracy_score(y_test,random_pred)}\")\n",
    "print(f\"acc of og : {accuracy_score(y_test,pred)}\")\n",
    "\n",
    "print(grid_search.best_params_)\n",
    "print(random_search.best_params_)\n",
    "print(dt.get_params())\n",
    "\n",
    "print(\"randomly change hyperparam\")\n",
    "dt.set_params(min_samples_leaf=2)\n",
    "dt.fit(X_train,y_train)\n",
    "print(f\"randomly change param : {accuracy_score(y_test,dt.predict(X_test))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "as you can see `GridSearchCV` and `Baseline` are the best, while `Baseline` could prone to overfit in the unseen data since there are no restriction on the decision tree.\n",
    "\n",
    "but as you can see, `RandomizedSearchCV` might not get the best result, since it just randomly search, it might or might not find the global minimum. And this case it doesn't\n",
    "\n",
    "and tuning hyperparameter definitely directly affect the result, I just change the `min_samples_split` and the accuracy changed. This is why we need to tune the hyperparameter and need to get the one that fit the use case."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "1a1af0ee75eeea9e2e1ee996c87e7a2b11a0bebd85af04bb136d915cefc0abce"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
